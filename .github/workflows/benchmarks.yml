# Benchmarks - Performance testing, profiling, and regression detection
# Runs Criterion benchmarks, generates flamegraphs, and tracks performance history

name: Benchmarks

on:
  push:
    branches: [master, main]
    paths:
      - "src/**"
      - "Cargo.toml"
      - "Cargo.lock"
      - "benches/**"
  pull_request:
    branches: [master, main]
    paths:
      - "src/**"
      - "Cargo.toml"
      - "Cargo.lock"
      - "benches/**"
  workflow_dispatch:
    inputs:
      baseline:
        description: 'Baseline commit/tag to compare against'
        required: false
        type: string
      run_profiling:
        description: 'Run profiling (flamegraph, memory analysis)'
        type: boolean
        default: false
  schedule:
    # Run weekly to track performance over time
    - cron: '0 0 * * 0'

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - uses: Swatinem/rust-cache@v2

      - name: Install cargo-criterion
        run: cargo install cargo-criterion --locked || true

      - name: Run NAT traversal benchmarks
        run: cargo bench --bench nat_traversal_performance -- --save-baseline current-nat || true

      - name: Run connection management benchmarks
        run: cargo bench --bench connection_management -- --save-baseline current-conn || true

      - name: Generate benchmark report
        run: |
          echo "# Benchmark Results" > benchmark-report.md
          echo "## NAT Traversal" >> benchmark-report.md
          cat target/criterion/nat_traversal_performance/*/base/estimates.json >> benchmark-report.md 2>/dev/null || echo "No results" >> benchmark-report.md

          # Create benchmark results JSON
          echo '{"timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'", "commit": "'${{ github.sha }}'", "results": []}' > benchmark-results.json
          if [ -d "target/criterion/nat_traversal_performance" ]; then
            find target/criterion/nat_traversal_performance -name "estimates.json" -exec cat {} \; | jq -s '.' > temp-results.json 2>/dev/null || echo "[]" > temp-results.json
            jq --argjson results "$(cat temp-results.json)" '.results = $results' benchmark-results.json > benchmark-results-final.json
            mv benchmark-results-final.json benchmark-results.json
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            benchmark-results.json
            benchmark-report.md
            target/criterion/

      - name: Download baseline benchmarks
        if: github.event_name == 'pull_request'
        uses: dawidd6/action-download-artifact@v6
        with:
          workflow: benchmarks.yml
          branch: ${{ github.base_ref }}
          name: benchmark-results-.*
          name_is_regexp: true
          path: baseline-benchmarks
          if_no_artifact_found: warn

      - name: Compare benchmarks
        if: github.event_name == 'pull_request' && success()
        id: compare
        run: |
          if [ -d "baseline-benchmarks" ]; then
            python3 .github/scripts/compare-benchmarks.py \
              baseline-benchmarks/*/benchmark-results.json \
              benchmark-results.json \
              > comparison-report.md 2>/dev/null || echo "Comparison script not available" > comparison-report.md

            if grep -q "REGRESSION" comparison-report.md; then
              echo "regression_found=true" >> $GITHUB_OUTPUT
            fi
          else
            echo "No baseline benchmarks found for comparison" > comparison-report.md
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let comment = '## Benchmark Results\n\n';

            if (fs.existsSync('comparison-report.md')) {
              comment += fs.readFileSync('comparison-report.md', 'utf8');
            } else {
              comment += fs.readFileSync('benchmark-report.md', 'utf8');
            }

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' &&
              c.body.includes('## Benchmark Results')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

      - name: Fail if regression found
        if: steps.compare.outputs.regression_found == 'true'
        run: |
          echo "Performance regression detected!"
          cat comparison-report.md
          exit 1

  profile:
    name: CPU Profiling
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.run_profiling == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - uses: Swatinem/rust-cache@v2

      - name: Install profiling tools
        run: |
          cargo install flamegraph --force || true
          sudo apt-get update
          sudo apt-get install -y linux-tools-common linux-tools-generic

      - name: Build with profiling
        run: cargo build --release

      - name: Generate flamegraph
        run: |
          sudo cargo flamegraph --root -- run --release --example simple_chat &
          FLAME_PID=$!
          sleep 10
          kill $FLAME_PID || true

      - name: Upload flamegraph
        uses: actions/upload-artifact@v4
        with:
          name: flamegraph
          path: flamegraph.svg

  memory-analysis:
    name: Memory Analysis
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.run_profiling == 'true'
    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - uses: Swatinem/rust-cache@v2

      - name: Install valgrind
        run: |
          sudo apt-get update
          sudo apt-get install -y valgrind

      - name: Build test binary
        run: cargo build --release --example simple_chat

      - name: Run valgrind analysis
        run: |
          timeout 30s valgrind \
            --leak-check=full \
            --show-leak-kinds=all \
            --track-origins=yes \
            --log-file=valgrind.log \
            ./target/release/examples/simple_chat || true

      - name: Parse valgrind results
        run: |
          if [ -f valgrind.log ]; then
            echo "### Memory Analysis Results" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            tail -n 50 valgrind.log >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload valgrind log
        uses: actions/upload-artifact@v4
        with:
          name: valgrind-log
          path: valgrind.log
        if: always()

  benchmark-history:
    name: Track History
    runs-on: ubuntu-latest
    # Allow this job to fail - benchmark history is optional and may fail on forks
    continue-on-error: true
    if: github.event_name == 'push' && (github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main')
    needs: benchmark
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download current results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}

      - name: Update benchmark history
        run: |
          mkdir -p .benchmarks
          cp benchmark-results.json .benchmarks/results-$(date +%Y%m%d-%H%M%S)-${{ github.sha }}.json

          # Keep only last 50 results
          ls -t .benchmarks/results-*.json | tail -n +51 | xargs rm -f || true

      - name: Commit benchmark history
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Update benchmark history [skip ci]"
          file_pattern: ".benchmarks/*.json"
          branch: benchmark-history
          create_branch: true
          push_options: '--force'
